
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstraSumm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yGm8kFqg1q7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0c80ec6a-614d-417a-9429-554ad4d75a5b"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "from nltk import ne_chunk\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import tensorflow as tf\n",
        "import re\n",
        "import heapq"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mLLjCqMI2Z9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5109a262-4f49-438f-9e5f-6744d275c386"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "f =open('/gdrive/My Drive/Colab_ML/Abstractive Summarizer/convotext.txt','r').read().lower()\n",
        "with tf.device('/device:GPU:0'):\n",
        "#     f = re.sub(r'\\s+', ' ', f)  \n",
        "    no_of_lines = len(open('/gdrive/My Drive/Colab_ML/Abstractive Summarizer/convotext.txt','r').readlines())\n",
        "    stop_words = set(stopwords.words('english'))\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DiXuminb32sL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ! cd \"/gdrive/My Drive/Colab_ML/Abstractive Summarizer\" && wget \"https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz\"\n",
        "# ! cd \"/gdrive/My Drive/Colab_ML/Abstractive Summarizer/\" && gunzip \"/gdrive/My Drive/Colab_ML/Abstractive Summarizer/numberbatch-en-17.06.txt.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NC7crxESFFAo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_punc(sent):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        punctuations = '''!()-[]{};'\"\\,<>./?@#%^&*_~'''\n",
        "        for x in sent: \n",
        "            if x in punctuations: \n",
        "                sent = sent.replace(x, \"\")\n",
        "        return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRxGIq7eDOBE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(sent):\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        sent = remove_punc(sent)\n",
        "        sent = nltk.word_tokenize(sent,language='english')\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        sent = [lemmatizer.lemmatize(x) for x in sent]\n",
        "        sent = ' '.join(sent)\n",
        "        filtered_sentence = [w for w in sent.split(' ') if not w in stop_words] \n",
        "                \n",
        "        return ' '.join(filtered_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4nkeS3wNpRv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weighted_freq(sent):\n",
        "    word_frequencies = {}  \n",
        "    for word in sent:  \n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "    \n",
        "    maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():  \n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "\n",
        "    return word_frequencies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XSphm4ZmOcnz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sent_score_calc(text,word_frequencies):\n",
        "    sentence_list = nltk.sent_tokenize(text)\n",
        "    sentence_scores = {}  \n",
        "    for sent in sentence_list:\n",
        "        for word in nltk.word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies.keys():\n",
        "                if len(sent.split(' ')) < 10:\n",
        "                    if sent not in sentence_scores.keys():\n",
        "                        sentence_scores[sesnt] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sent] += word_frequencies[word]\n",
        "    return sentence_scores\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPVVfxPmo0pD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d3bd7ae6-7d20-44e8-b7d7-4e20cdda8d4c"
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "    nlp = en_core_web_sm.load()\n",
        "    docu = preprocess(f)\n",
        "    doc = nlp(docu)\n",
        "#     pprint([(X.text, X.label_) for X in doc.ents])\n",
        "#     pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])\n",
        "#     displacy.render((doc), jupyter=True, style='ent')\n",
        "    max_freq=weighted_freq(docu)\n",
        "    sent_scores = sent_score_calc(f,max_freq)\n",
        "    summary_sentences = heapq.nlargest(int(no_of_lines/2), sent_scores, key=sent_scores.get)\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    print(summary.capitalize()) "
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "As i said, i'll call you back. tim:\tah, it’s just me. dolores:\tyes, sure, i understand. dolores:\t\n",
            "can i help you with anything else? tim:\tand is there a discount rate for conference delegates? can i book that, then? i just need to check one or two details. can i help you with anything else? dolores:\tcertainly. dolores:\tno, tax is another $70 on top of that. dolores:\thello! dolores speaking…\n",
            "tim:\tah yes, hello. tim:\tand how much will that be?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cDX7YrwIGK6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
