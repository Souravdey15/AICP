{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "yGm8kFqg1q7B",
    "outputId": "0c80ec6a-614d-417a-9429-554ad4d75a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from nltk import ne_chunk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "mLLjCqMI2Z9e",
    "outputId": "5109a262-4f49-438f-9e5f-6744d275c386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "f =open('/gdrive/My Drive/Colab_ML/Abstractive Summarizer/convotext.txt','r').read().lower()\n",
    "with tf.device('/device:GPU:0'):\n",
    "#     f = re.sub(r'\\s+', ' ', f)  \n",
    "    no_of_lines = len(open('/gdrive/My Drive/Colab_ML/Abstractive Summarizer/convotext.txt','r').readlines())\n",
    "    stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiXuminb32sL"
   },
   "outputs": [],
   "source": [
    "# ! cd \"/gdrive/My Drive/Colab_ML/Abstractive Summarizer\" && wget \"https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz\"\n",
    "# ! cd \"/gdrive/My Drive/Colab_ML/Abstractive Summarizer/\" && gunzip \"/gdrive/My Drive/Colab_ML/Abstractive Summarizer/numberbatch-en-17.06.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC7crxESFFAo"
   },
   "outputs": [],
   "source": [
    "def remove_punc(sent):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        punctuations = '''!()-[]{};'\"\\,<>./?@#%^&*_~'''\n",
    "        for x in sent: \n",
    "            if x in punctuations: \n",
    "                sent = sent.replace(x, \"\")\n",
    "        return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRxGIq7eDOBE"
   },
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        sent = remove_punc(sent)\n",
    "        sent = nltk.word_tokenize(sent,language='english')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        sent = [lemmatizer.lemmatize(x) for x in sent]\n",
    "        sent = ' '.join(sent)\n",
    "        filtered_sentence = [w for w in sent.split(' ') if not w in stop_words] \n",
    "                \n",
    "        return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4nkeS3wNpRv"
   },
   "outputs": [],
   "source": [
    "def weighted_freq(sent):\n",
    "    word_frequencies = {}  \n",
    "    for word in sent:  \n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "    \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSphm4ZmOcnz"
   },
   "outputs": [],
   "source": [
    "def sent_score_calc(text,word_frequencies):\n",
    "    sentence_list = nltk.sent_tokenize(text)\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 10:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sesnt] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "    return sentence_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "lPVVfxPmo0pD",
    "outputId": "d3bd7ae6-7d20-44e8-b7d7-4e20cdda8d4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As i said, i'll call you back. tim:\tah, it’s just me. dolores:\tyes, sure, i understand. dolores:\t\n",
      "can i help you with anything else? tim:\tand is there a discount rate for conference delegates? can i book that, then? i just need to check one or two details. can i help you with anything else? dolores:\tcertainly. dolores:\tno, tax is another $70 on top of that. dolores:\thello! dolores speaking…\n",
      "tim:\tah yes, hello. tim:\tand how much will that be?\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    docu = preprocess(f)\n",
    "    doc = nlp(docu)\n",
    "#     pprint([(X.text, X.label_) for X in doc.ents])\n",
    "#     pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])\n",
    "#     displacy.render((doc), jupyter=True, style='ent')\n",
    "    max_freq=weighted_freq(docu)\n",
    "    sent_scores = sent_score_calc(f,max_freq)\n",
    "    summary_sentences = heapq.nlargest(int(no_of_lines/2), sent_scores, key=sent_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    print(summary.capitalize()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDX7YrwIGK6M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "abstraSumm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
